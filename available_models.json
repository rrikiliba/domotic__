[
    {
        "id": "google/gemma-3-27b-it:free",
        "canonical_slug": "google/gemma-3-27b-it",
        "hugging_face_id": "",
        "name": "Google: Gemma 3 27B (free)",
        "created": 1741756359,
        "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 27B is Google's latest open source model, successor to [Gemma 2](google/gemma-2-27b-it)",
        "context_length": 131072,
        "architecture": {
            "modality": "text+image->text",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "tokenizer": "Gemini",
            "instruct_type": "gemma"
        },
        "pricing": {
            "prompt": "0",
            "completion": "0",
            "request": "0",
            "image": "0",
            "web_search": "0",
            "internal_reasoning": "0"
        },
        "top_provider": {
            "context_length": 131072,
            "max_completion_tokens": null,
            "is_moderated": false
        },
        "per_request_limits": null,
        "supported_parameters": [
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "seed",
            "structured_outputs",
            "temperature",
            "top_p"
        ],
        "default_parameters": {}
    },
    {
        "id": "mistralai/mistral-7b-instruct:free",
        "canonical_slug": "mistralai/mistral-7b-instruct",
        "hugging_face_id": "mistralai/Mistral-7B-Instruct-v0.3",
        "name": "Mistral: Mistral 7B Instruct (free)",
        "created": 1716768000,
        "description": "A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.\n\n*Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*",
        "context_length": 32768,
        "architecture": {
            "modality": "text->text",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "tokenizer": "Mistral",
            "instruct_type": "mistral"
        },
        "pricing": {
            "prompt": "0",
            "completion": "0",
            "request": "0",
            "image": "0",
            "web_search": "0",
            "internal_reasoning": "0"
        },
        "top_provider": {
            "context_length": 32768,
            "max_completion_tokens": 16384,
            "is_moderated": false
        },
        "per_request_limits": null,
        "supported_parameters": [
            "frequency_penalty",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_p"
        ],
        "default_parameters": {
            "temperature": 0.3
        }
    },
    {
        "id": "tngtech/deepseek-r1t2-chimera:free",
        "canonical_slug": "tngtech/deepseek-r1t2-chimera",
        "hugging_face_id": "tngtech/DeepSeek-TNG-R1T2-Chimera",
        "name": "TNG: DeepSeek R1T2 Chimera (free)",
        "created": 1751986985,
        "description": "DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2× faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.",
        "context_length": 163840,
        "architecture": {
            "modality": "text->text",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "tokenizer": "DeepSeek",
            "instruct_type": null
        },
        "pricing": {
            "prompt": "0",
            "completion": "0",
            "request": "0",
            "image": "0",
            "web_search": "0",
            "internal_reasoning": "0"
        },
        "top_provider": {
            "context_length": 163840,
            "max_completion_tokens": null,
            "is_moderated": false
        },
        "per_request_limits": null,
        "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "max_tokens",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "top_k",
            "top_p"
        ],
        "default_parameters": {}
    },
    {
        "id": "meta-llama/llama-3.3-70b-instruct:free",
        "canonical_slug": "meta-llama/llama-3.3-70b-instruct",
        "hugging_face_id": "meta-llama/Llama-3.3-70B-Instruct",
        "name": "Meta: Llama 3.3 70B Instruct (free)",
        "created": 1733506137,
        "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)",
        "context_length": 131072,
        "architecture": {
            "modality": "text->text",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "tokenizer": "Llama3",
            "instruct_type": "llama3"
        },
        "pricing": {
            "prompt": "0",
            "completion": "0",
            "request": "0",
            "image": "0",
            "web_search": "0",
            "internal_reasoning": "0"
        },
        "top_provider": {
            "context_length": 131072,
            "max_completion_tokens": null,
            "is_moderated": false
        },
        "per_request_limits": null,
        "supported_parameters": [
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "repetition_penalty",
            "stop",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_p"
        ],
        "default_parameters": {}
    }
]